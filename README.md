
# 求解 Lasso 回归算法分析

## 一、引言

Lasso（Least Absolute Shrinkage and Selection Operator）回归通过在损失函数中引入 L1 正则化项，能够实现特征选择和模型稀疏化，广泛应用于高维数据建模。本实验采用标准 Lasso 目标函数，数学形式为：

$$
\min_{\beta} \frac{1}{2n} \| X\beta - y\|_2^2 + \lambda \|\beta\|_1
$$

其中：
- $X \in \mathbb{R}^{n \times p}$ 为特征矩阵（$n$ 为样本数，$p$ 为特征数）
- $y \in \mathbb{R}^n$ 为标签向量
- $\lambda > 0$ 为正则化参数（控制稀疏化程度）
- $\beta \in \mathbb{R}^p$ 为待估计的系数向量
- $\frac{1}{2n}$ 为残差平方和的归一化因子

由于 L1 正则化项的非光滑性，传统梯度下降法难以直接应用，本文旨在通过实验比较 5 类主流 Lasso 求解算法的性能，重点分析不同 $(n,p)$ 组合对算法计算效率和求解精度的影响，为算法选择提供参考。

---

## 二、实验设置

### 1. 测试算法

本次实验实现 5 类 11 种算法：

1. **最速坐标下降法**（Coordinate Desc）
2. **临近点梯度法**（Proximal Gradient, ISTA）
3. **标准 FISTA**、**带重启的 FISTA**（FISTA (Restart)）
4. **ADMM**（$\rho = 0.5, 1, 2, 5$）
5. **Huber 梯度法**、**加速 Huber 梯度法**、**带重启的加速 Huber 梯度法**

所有算法的迭代终止条件均为：达到最大迭代次数，未设置基于精度的终止条件，确保所有算法在相同迭代步数下进行公平比较。

### 2. 数据生成

- **真实系数向量 $\beta_{\text{true}}$**：稀疏度为 10%（即 $p \times 0.1$ 个非零元素），非零元素服从标准正态分布 $\mathcal{N}(0,1)$
- **特征矩阵 $X$**：每个元素独立服从标准正态分布 $\mathcal{N}(0,1)$，生成后对每列进行标准化处理
- **标签向量 $y$**：$y = X\beta_{\text{true}} + \epsilon$，其中噪声项 $\epsilon \sim \mathcal{N}(0,0.1^2)$
- **数据标准化**：特征矩阵 $X$ 和标签向量 $y$ 均进行标准化处理，确保数值稳定性

### 3. 实验参数

- **独立重复实验次数**：$n_{\text{trials}} = 100$
- **最大迭代次数**：$\max_{\text{iter}} = 75$
- **正则化参数**：$\lambda = 0.1$
- **Huber 平滑参数**：$\mu = 0.01$
- **ADMM 惩罚参数**：$\rho \in \{0.5, 1, 2, 5\}$
- **最优解参考**：使用 scikit-learn 库的 Lasso 实现（max_iter=10000，tol = $1 \times 10^{-8}$）计算最优目标值 $f^*$，作为次优性计算的基准

**维度组合选择**：

1. **低维场景**：$n=200$，$p=50$（$p < n$，特征数远小于样本数，传统统计学习场景）
2. **中维场景**：$n=1000$，$p=200$（$p < n$，样本数和特征数均较大，常规机器学习场景）
3. **高维场景**：$n=500$，$p=1000$（$p > n$，特征数大于样本数，高维小样本场景，Lasso 的典型应用场景）

### 4. 评价指标

采用两个核心评价指标：

1. **最终次优性**：$f(\beta_k) - f^*$，其中 $f(\beta)$ 为目标函数值，反映算法在最大迭代步数下的求解精度
2. **计算时间**：单次实验的总运行时间（单位：秒），反映算法的计算效率

---

## 三、实验结果与分析

### 1. 低维场景（$n = 200, p = 50$）

#### (1) 收敛性分析

![Lasso 收敛云雾图](lasso_convergence_cloud_100_trials（1000，200）[1].png) 

收敛性分析如下：
- 除 Huber 类算法外，其余算法（坐标下降法、临近点梯度法、FISTA 系列、ADMM 系列）均能在 75 次迭代内收敛到最优解（次优性接近 $1 \times 10^{-10}$）
- ADMM 系列算法收敛速度最快，在 10 次迭代内即可达到最优解，且不同 $\rho$ 值的 ADMM 性能差异较小
- FISTA 系列和临近点梯度法收敛速度次之，约在 20-30 次迭代内收敛
- Huber 类算法收敛后存在固定次优性误差（约 $4.3 \times 10^{-3}$），这是由于 Huber 函数对 L1 正则项的平滑近似导致的，平滑误差无法通过增加迭代次数消除
- 坐标下降法收敛速度最慢，但最终能达到最优解

#### (2) 性能统计

![次优性箱线图/时间计算散点图](lasso_performance_comparison_100_trials（1000，200）[1].png) 

**表 1：低维场景(200, 50) 算法性能统计**

| 算法名称 | 平均最终次优性 | 次优性标准差 | 平均计算时间（秒） | 时间标准差 | 中位数时间（秒） |
|----------|----------------|--------------|-------------------|------------|------------------|
| Coordinate Desc | $0.000000$ | $0.000000$ | $0.075320$ | $0.003216$ | $0.074994$ |
| Proximal Gradient (ISTA) | $0.000000$ | $0.000000$ | $0.005907$ | $0.000814$ | $0.005980$ |
| FISTA | $0.000000$ | $0.000000$ | $0.006008$ | $0.000535$ | $0.005981$ |
| FISTA (Restart) | $0.000000$ | $0.000000$ | $0.006054$ | $0.000456$ | $0.005981$ |
| ADMM ($\rho=0.5$) | $0.000000$ | $0.000000$ | $0.004614$ | $0.000594$ | $0.004853$ |
| ADMM ($\rho=1$) | $0.000000$ | $0.000000$ | $0.004492$ | $0.000558$ | $0.004179$ |
| ADMM ($\rho=2$) | $0.000000$ | $0.000000$ | $0.004580$ | $0.000649$ | $0.004427$ |
| ADMM ($\rho=5$) | $0.000000$ | $0.000000$ | $0.004570$ | $0.000795$ | $0.004317$ |
| Huber Gradient | $0.004300$ | $0.000448$ | $0.005785$ | $0.000683$ | $0.005980$ |
| Accelerated Huber Gradient | $0.004271$ | $0.000439$ | $0.006182$ | $0.000530$ | $0.005981$ |
| Restarted Accelerated Huber Gradient | $0.004264$ | $0.000436$ | $0.009425$ | $0.000633$ | $0.009282$ |

**表 2：低维场景下不同优化算法的性能对比结论**

| 评估指标 | 算法类别/名称 | 关键结论 |
|----------|--------------|----------|
| 求解精度 | 坐标下降法、临近点梯度法、FISTA 系列、ADMM 系列 | 均能达到最优解，平均次优性为 $0$ |
| | Huber 类算法（标准/带重启加速） | 存在固定次优性误差；带重启的加速 Huber 梯度法误差最小（$4.264 \times 10^{-3}$） |
| 计算速度 | ADMM 系列（$\rho=1$ 最优） | 速度最快，平均时间 $0.0045 \sim 0.0046$ 秒；ADMM（$\rho=1$）最快且稳定性最好 |
| | 临近点梯度法、FISTA 系列、标准 Huber 梯度法 | 速度相近，平均时间 $0.0058 \sim 0.0062$ 秒 |
| | 坐标下降法 | 速度最慢（$0.0753$ 秒），是最快算法的 $16.8$ 倍 |

---

## 四、结论与建议

### 1. 实验结论

通过对三种维度场景的 $100$ 次独立实验，得出以下结论：

- **ADMM 系列算法**（尤其是 $\rho = 1, 2, 5$）在所有维度场景下均表现最优，兼顾求解精度（达到最优解）和计算效率（最快速度），且对维度变化的适应性最强，是标准 Lasso 回归的首选算法
- **FISTA 系列算法**在所有场景下均能达到最优解，高维场景下速度仅次于 ADMM，是实现相对简单且性能稳定的替代选择
- **临近点梯度法**在中低维场景下性能稳定，精度达到最优解，但在高维场景下速度下降明显
- **Huber 类算法**求解速度中等，但存在显著次优性误差，且误差随维度增加急剧增大，不适合精度要求较高的场景
- **坐标下降法**在所有场景下虽能达到最优解，但计算效率极低，且在高维场景下时间成本不可接受，不推荐用于实际应用
- **维度变化对不同算法的影响差异显著**：ADMM 系列受影响最小，坐标下降法受影响最大，Huber 类算法在精度上受维度影响最大

### 2. 应用建议

- **高维小样本场景（$p > n$）**：优先选择 ADMM（$\rho = 5$ 或 $\rho = 1$），兼顾精度和速度；FISTA 系列是良好的备选方案
- **中维场景（$p < n$ 且样本数较大）**：ADMM 系列仍为最优选择；FISTA 系列和临近点梯度法也是良好选择
- **低维场景（$p < n$ 且样本数较小）**：ADMM 系列（追求速度）或 FISTA 系列（追求实现简单）都是良好选择
- **算法参数选择**：ADMM 的惩罚参数 $\rho$ 建议选择 $1 \sim 5$，Huber 平滑参数 $\mu$ 需要根据维度调整，高维场景下可能需要更小的 $\mu$ 值

---
